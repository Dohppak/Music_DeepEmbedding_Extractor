{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.plotting import output_notebook, output_file, reset_output\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = \"./dataset/NIPS_Workshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BohemianRhapsody = ['BohemianRhapsody_original.mp3','BohemianRhapsody_female.mp3']\n",
    "UPtownFunk = ['UptownFunk_original.mp3','UptownFunk_female.mp3']\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length, model, checkpoint_path = load_model(59049, \"FCN037\")\n",
    "model = model.to(device)\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "new_state_map = {model_key: model_key.split(\"model.\")[1] for model_key in state_dict.get(\"state_dict\").keys()}\n",
    "new_state_dict = {new_state_map[key]: value for (key, value) in state_dict.get(\"state_dict\").items() if key in new_state_map.keys()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(mp3_path):\n",
    "    waveform, sr = torchaudio.load(mp3_path)\n",
    "    downsample_resample = torchaudio.transforms.Resample(sr, 16000)\n",
    "    audio_tensor = downsample_resample(waveform)\n",
    "    audio_tensor = torch.mean(audio_tensor, dim=0)\n",
    "    return audio_tensor, len(audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(audio_tensor, audio_length, input_length, sampleing_rate = 16000):\n",
    "    num_frame = int(audio_length / input_length)\n",
    "    hop_size = int(sampleing_rate / 15)\n",
    "    split = [audio_tensor[i:i+input_length] for i in range(0,audio_length-input_length, hop_size)]\n",
    "    batch_audio = torch.stack(split[:-1])\n",
    "    return batch_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_embeddings(mp3_path, model, input_length=input_length):\n",
    "    results = []\n",
    "    audio, audio_length = get_audio(os.path.join(sample_dir,mp3_path))\n",
    "    batch_audio = make_frames(audio, audio_length, input_length)\n",
    "    batch_audio = torch.split(batch_audio, 16)\n",
    "    for i in batch_audio:\n",
    "        batch_results = []\n",
    "        with torch.no_grad():\n",
    "            _, embeddings = model(i.to(device))\n",
    "            batch_results.extend(embeddings.detach().cpu().numpy())\n",
    "        results.append(batch_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_origin_embedding = get_frame_embeddings(\"BohemianRhapsody_original.mp3\", model, input_length=input_length)\n",
    "B_origin_embedding = [instance for batch in B_origin_embedding for instance in batch]\n",
    "B_origin_embedding = np.stack(B_origin_embedding)\n",
    "B_origin_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPC Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
